{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12228470,"sourceType":"datasetVersion","datasetId":7704570}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets peft accelerate bitsandbytes torch\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:18:42.344059Z","iopub.execute_input":"2025-06-20T13:18:42.344373Z","iopub.status.idle":"2025-06-20T13:18:49.262750Z","shell.execute_reply.started":"2025-06-20T13:18:42.344348Z","shell.execute_reply":"2025-06-20T13:18:49.261699Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.46.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Step 2: Import libraries\nfrom datasets import Dataset\nimport json\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    TrainingArguments, \n    Trainer,\n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model\nimport torch\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:19:12.510257Z","iopub.execute_input":"2025-06-20T13:19:12.510600Z","iopub.status.idle":"2025-06-20T13:19:35.119097Z","shell.execute_reply.started":"2025-06-20T13:19:12.510552Z","shell.execute_reply":"2025-06-20T13:19:35.118423Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nimport os\nos.environ[\"HF_TOKEN\"]=\"\"\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"  # Force-disable wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:20:17.588922Z","iopub.execute_input":"2025-06-20T13:20:17.589552Z","iopub.status.idle":"2025-06-20T13:20:17.593288Z","shell.execute_reply.started":"2025-06-20T13:20:17.589523Z","shell.execute_reply":"2025-06-20T13:20:17.592407Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load the dataset:\n\n\n\n# Step 3: Load and prepare dataset\nwith open(\"/kaggle/input/datawalm/walmart-qa.json\") as f:\n    data = json.load(f)\n\n# Format as instruction-following data\nformatted_data = [\n    {\"text\": f\"### Instruction: {d['question']}\\n### Response: {d['answer']}\"}\n    for d in data\n]\n\ndataset = Dataset.from_list(formatted_data)\ndataset = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:21:18.249368Z","iopub.execute_input":"2025-06-20T13:21:18.249726Z","iopub.status.idle":"2025-06-20T13:21:18.282106Z","shell.execute_reply.started":"2025-06-20T13:21:18.249699Z","shell.execute_reply":"2025-06-20T13:21:18.281475Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Step 4: Load tokenizer\nmodel_id = \"meta-llama/Llama-3.2-1B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n    tokenized = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=512,  # Increased for better context\n        padding=\"max_length\"\n    )\n    # Add labels (shifted input_ids for causal LM)\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:21:39.218347Z","iopub.execute_input":"2025-06-20T13:21:39.218703Z","iopub.status.idle":"2025-06-20T13:21:41.489910Z","shell.execute_reply.started":"2025-06-20T13:21:39.218673Z","shell.execute_reply":"2025-06-20T13:21:41.488964Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e704927ed294497a1d080dda54e3a1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"366ea0d0e83b4b84922f14983f33c8d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f32290fb55604dca8582a1e5c94f81ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca92c10602ab4d8e80be22abca928451"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1958524b43664de4bd9598afc5aca121"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Step 5: Configure 4-bit QLoRA\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\nlora_config = LoraConfig(\n    r=16,  # Higher rank for better adaptation\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:22:10.544139Z","iopub.execute_input":"2025-06-20T13:22:10.544474Z","iopub.status.idle":"2025-06-20T13:22:10.550362Z","shell.execute_reply.started":"2025-06-20T13:22:10.544443Z","shell.execute_reply":"2025-06-20T13:22:10.549654Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Step 6: Load model with QLoRA\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",  # Automatically uses GPU\n    torch_dtype=torch.float16\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # Should show ~0.1% parameters\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:23:38.723074Z","iopub.execute_input":"2025-06-20T13:23:38.723401Z","iopub.status.idle":"2025-06-20T13:23:54.125287Z","shell.execute_reply.started":"2025-06-20T13:23:38.723377Z","shell.execute_reply":"2025-06-20T13:23:54.124497Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c005536df7d8419e95ba6cf794bb0d2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bbaa5f4e2224cedb2ebfe2652bdf439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"865cfa93aaa54b4cbfe269af2527291e"}},"metadata":{}},{"name":"stdout","text":"trainable params: 3,407,872 || all params: 1,239,222,272 || trainable%: 0.2750\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Step 7: Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:24:34.384974Z","iopub.execute_input":"2025-06-20T13:24:34.385352Z","iopub.status.idle":"2025-06-20T13:24:34.389104Z","shell.execute_reply.started":"2025-06-20T13:24:34.385319Z","shell.execute_reply":"2025-06-20T13:24:34.388176Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Step 8: Training arguments (GPU optimized)\ntraining_args = TrainingArguments(\n    output_dir=\"./walmart-llama3-results\",\n    per_device_train_batch_size=8,  # Higher batch size for GPU\n    gradient_accumulation_steps=2,\n    num_train_epochs=3,\n    learning_rate=2e-4,  # Higher learning rate for QLoRA\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n    report_to=\"none\",\n    remove_unused_columns=False,\n    fp16=True,  # Mixed precision training\n    optim=\"paged_adamw_8bit\",  # Optimized for 8-bit training\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:24:55.964781Z","iopub.execute_input":"2025-06-20T13:24:55.965083Z","iopub.status.idle":"2025-06-20T13:24:55.995117Z","shell.execute_reply.started":"2025-06-20T13:24:55.965059Z","shell.execute_reply":"2025-06-20T13:24:55.994339Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Step 9: Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:25:17.853109Z","iopub.execute_input":"2025-06-20T13:25:17.853412Z","iopub.status.idle":"2025-06-20T13:25:17.871069Z","shell.execute_reply.started":"2025-06-20T13:25:17.853390Z","shell.execute_reply":"2025-06-20T13:25:17.870318Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-10-8a801911bb95>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Step 10: Train and save\nprint(\"Starting training...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:25:28.968546Z","iopub.execute_input":"2025-06-20T13:25:28.968869Z","iopub.status.idle":"2025-06-20T13:25:48.160431Z","shell.execute_reply.started":"2025-06-20T13:25:28.968846Z","shell.execute_reply":"2025-06-20T13:25:48.159558Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:11, Epoch 1/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.418840</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3, training_loss=5.283638636271159, metrics={'train_runtime': 18.8105, 'train_samples_per_second': 3.509, 'train_steps_per_second': 0.159, 'total_flos': 113999005876224.0, 'train_loss': 5.283638636271159, 'epoch': 1.6666666666666665})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Save adapter only (small file)\nmodel.save_pretrained(\"walmart-llama3-adapter\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:26:25.606045Z","iopub.execute_input":"2025-06-20T13:26:25.606390Z","iopub.status.idle":"2025-06-20T13:26:25.847623Z","shell.execute_reply.started":"2025-06-20T13:26:25.606363Z","shell.execute_reply":"2025-06-20T13:26:25.846552Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Step 11: Inference\ndef generate_response(question):\n    prompt = f\"### Instruction: {question}\\n### Response:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=128,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.1\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:26:45.776123Z","iopub.execute_input":"2025-06-20T13:26:45.776429Z","iopub.status.idle":"2025-06-20T13:26:45.780973Z","shell.execute_reply.started":"2025-06-20T13:26:45.776405Z","shell.execute_reply":"2025-06-20T13:26:45.779953Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Test\nprint(\"\\nGenerated response:\")\nprint(generate_response(\"What is Walmart's price match policy?\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:26:57.266488Z","iopub.execute_input":"2025-06-20T13:26:57.266814Z","iopub.status.idle":"2025-06-20T13:27:04.233141Z","shell.execute_reply.started":"2025-06-20T13:26:57.266788Z","shell.execute_reply":"2025-06-20T13:27:04.232369Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated response:\n### Instruction: What is Walmart's price match policy?\n### Response: Walmart does not offer a refund or credit for items that were returned because they did not fit the customer. The company states it will refund customers who have merchandise that was purchased at full retail and then found to be defective in material, workmanship or quality.\n#### Explanation:\nWalmart does not accept returns on items with physical defects even if the item has been used. A company spokesperson said it would not honor returns for items that are too small or too large. Items that do not meet size requirements may still be eligible for return under certain circumstances depending upon the nature of the defect. For example, an item that does not meet its intended use\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}